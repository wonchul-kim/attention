# attention

## Attention is All You Need

* 기계번역 신경망으로 self-attentin 이란 기법을 사용하여 기존의 RNN 또는 CNN 계열의 신경망 구조를 탈피하려고 함
* 기존 기계번역 
  * CNN: 
    $h_t = 
